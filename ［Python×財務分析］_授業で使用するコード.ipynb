{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "［Python×財務分析］ 授業で使用するコード.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imaiyas/hello-world/blob/master/%EF%BC%BBPython%C3%97%E8%B2%A1%E5%8B%99%E5%88%86%E6%9E%90%EF%BC%BD_%E6%8E%88%E6%A5%AD%E3%81%A7%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B%E3%82%B3%E3%83%BC%E3%83%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSXAzPpDW-sL",
        "colab_type": "text"
      },
      "source": [
        "#Googleドライブをマウントする\n",
        "Googleドライブにファイルを保存して、再利用できるようにする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywr_-9UHW-U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "PROJECT_NAME = 'EDINET Scraping'\n",
        "BASE_DIR = f'/content/drive/My Drive/Colab Notebooks/data/{ PROJECT_NAME }/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-MVi8gc5i0",
        "colab_type": "text"
      },
      "source": [
        "# １．yuhoキャッチャーからURLを取得\n",
        "XBRLのURLリスト dat_download{ str_period }.csv が吐き出される"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZyaRiITvejI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os ,csv ,time ,re ,requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime ,timedelta\n",
        "\n",
        "class yuho_catcher() :\n",
        "  def __init__( self ,since ,until ,base_dir=None ) :\n",
        "      self.csv_tag = [ 'id' ,'title' ,'cd' ,'url' ,'update' ]\n",
        "      self.encode_type = 'utf-8'\n",
        "      self.wait_time = 2 #間隔が短いと制限がかかる\n",
        "      self.base_url = 'http://resource.ufocatch.com/atom/edinetx/'\n",
        "      self.namespace = '{http://www.w3.org/2005/Atom}'\n",
        "      self.out_of_since = False\n",
        "      self.since = since\n",
        "      self.until = until\n",
        "      self.file_info_str = since.strftime( '_%y%m%d_' ) + until.strftime( '%y%m%d' )\n",
        "      self.base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n",
        "\n",
        "  def get_link_info_str( self ,ticker_symbol ) :\n",
        "      url = self.base_url + str( ticker_symbol )\n",
        "      count ,retry = 0 ,3\n",
        "      while True:\n",
        "          try : \n",
        "              response = requests.get( url )\n",
        "              return response.text\n",
        "          except Exception :\n",
        "              print( f'{ticker_symbol} のアクセスに失敗しました。[ {count} ]' )\n",
        "              if count < retry : \n",
        "                  count += 1\n",
        "                  time.sleep( 3 )\n",
        "                  continue\n",
        "              else : raise\n",
        "    \n",
        "  def parse_xml( self ,string ) :\n",
        "      ET_tree = ET.fromstring( string )\n",
        "      ET.register_namespace( '' ,self.namespace[1:-1] )\n",
        "      return ET_tree\n",
        "\n",
        "  def get_link( self ,tree ) :\n",
        "      yuho_dict = {}\n",
        "      # xmlのentry毎にfor\n",
        "      for el in tree.findall('.//'+self.namespace+'entry'):\n",
        "          title = el.find( self.namespace+'title' ).text\n",
        "          if not self.is_yuho( title ) : continue\n",
        "          updated = el.find( self.namespace+'updated' ).text\n",
        "          checked = self.time_check( updated )\n",
        "          if not checked[ 'until' ] : continue\n",
        "          if not checked[ 'since' ] : \n",
        "              self.out_of_since = True\n",
        "              return yuho_dict\n",
        "          _id = el.find( self.namespace+'id' ).text\n",
        "          links = el.findall( './'+self.namespace+'link[@type=\"text/xml\"]' )\n",
        "          for link in links :\n",
        "              if '.xbrl' in link.attrib[ 'href' ] and 'PublicDoc' in link.attrib[ 'href' ] : \n",
        "                  url = link.attrib[ 'href' ]\n",
        "                  break\n",
        "              else : continue\n",
        "          else : continue\n",
        "          cd = re.sub( r'^【(\\w+)】.*',r\"\\1\" ,title )\n",
        "          yuho_dict[_id] = { 'id':_id ,'title':title ,'cd':cd ,'url':url ,'update':updated }\n",
        "      return yuho_dict\n",
        "\n",
        "  def is_yuho( self ,title ) :\n",
        "      if  u'有価証券報告書' in str(title) and u'株式会社' in str(title) and u'内国投資信託受益証券' not in str(title) :\n",
        "          return True\n",
        "      return False\n",
        "\n",
        "  def time_check( self ,update ) :\n",
        "      updated_time = datetime.strptime( update ,'%Y-%m-%dT%H:%M:%S+09:00' )\n",
        "      return { 'since':updated_time >= self.since ,'until':updated_time < self.until }\n",
        "    \n",
        "  def dump_file( self ,file ,info_dict ,tag ,encode_type ) :\n",
        "      with open( os.path.join( self.base_path ,file ) ,'w' ,encoding=encode_type ) as of :\n",
        "          writer = csv.DictWriter( of ,tag ,lineterminator='\\n' )\n",
        "          writer.writeheader()\n",
        "          for key in info_dict : writer.writerow( info_dict[key] )\n",
        "              \n",
        "  def craete_xbrl_url_json_each_symbols( self ,list_symbols ) :\n",
        "      print( f'since:{ self.since } ,until:{ self.until } ({ self.file_info_str })' )\n",
        "      i ,result_dict = 0 ,{}\n",
        "      for t_symbol in tqdm( list_symbols ) :\n",
        "          response_string = self.get_link_info_str( f'query/{t_symbol}' )\n",
        "          ET_tree = self.parse_xml( response_string )\n",
        "          info_dict = self.get_link( ET_tree )\n",
        "          if len( info_dict ) > 0 : \n",
        "              for key in info_dict : result_dict[key] = info_dict[key]\n",
        "              self.dump_file( f'dat_download{ self.file_info_str }.csv' ,result_dict ,self.csv_tag ,self.encode_type )\n",
        "          time.sleep( self.wait_time )\n",
        "          i += 1\n",
        "      print( 'complete a download!!' )\n",
        "      \n",
        "  def craete_xbrl_url_json_from_latest( self ) :\n",
        "      print( f'since:{ self.since } ,until:{ self.until } ({ self.file_info_str })' )\n",
        "      i ,result_dict = 0 ,{}\n",
        "      while True :\n",
        "          page = 1 + i\n",
        "          print( f'page{page}, loading...' )\n",
        "          response_string = self.get_link_info_str( page )\n",
        "          ET_tree = self.parse_xml( response_string )\n",
        "          info_dict = self.get_link( ET_tree )\n",
        "          if len( info_dict ) > 0 : \n",
        "              for key in info_dict : result_dict[key] = info_dict[key]\n",
        "              self.dump_file( f'dat_download{ self.file_info_str }.csv' ,result_dict ,self.csv_tag ,self.encode_type )\n",
        "          time.sleep( self.wait_time )\n",
        "          if self.out_of_since : break\n",
        "          i += 1\n",
        "      print( 'complete a download!!' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9fbANxfdKH7",
        "colab_type": "code",
        "outputId": "ec15b2d0-6355-405f-d982-f6525709ef17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "since = datetime.strptime('2020-04-01' ,'%Y-%m-%d')\n",
        "until = datetime.strptime('2020-05-01' ,'%Y-%m-%d')\n",
        "# yuho = yuho_catcher( since ,until ,base_dir=BASE_DIR )\n",
        "yuho = yuho_catcher( since ,until )\n",
        "yuho.craete_xbrl_url_json_from_latest()\n",
        "# yuho.craete_xbrl_url_json_each_symbols( [7203] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "since:2020-04-01 00:00:00 ,until:2020-05-01 00:00:00 (_200401_200501)\n",
            "page1, loading...\n",
            "page2, loading...\n",
            "page3, loading...\n",
            "page4, loading...\n",
            "page5, loading...\n",
            "page6, loading...\n",
            "page7, loading...\n",
            "page8, loading...\n",
            "page9, loading...\n",
            "page10, loading...\n",
            "page11, loading...\n",
            "complete a download!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAOoZ3OSX4Bc",
        "colab_type": "text"
      },
      "source": [
        "#２．XBRLファイルをDLしてCSVに変換\n",
        "XBRLの要素リスト egg_{ str_period }.csv が吐き出される"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qKejfmlYCoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install python-xbrl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1X-VunTYDv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os ,re ,csv ,time ,urllib\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime ,timedelta\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from xbrl import XBRLParser\n",
        "\n",
        "default_tag = ['file_nm','element_id','amount']\n",
        "custom_tag  = ['unit_ref','decimals','contextref']\n",
        "encode_type = 'utf-8'\n",
        "\n",
        "\n",
        "\n",
        "class XbrlParser( XBRLParser ):\n",
        "    def __init__( self ,info_dict ):\n",
        "        self.info_dict = info_dict\n",
        "        self.xbrl_url = info_dict[ 'url' ]\n",
        "        xbrl_filename = self.xbrl_url.split('/')[-1]\n",
        "        directory_path = f'{ os.getcwd() }/xbrl_files'\n",
        "        self.__make_directory( directory_path )\n",
        "        self.xbrl_filepath = f'{ directory_path }/{ xbrl_filename }.xbrl'\n",
        "\n",
        "    def __make_directory( self ,dir_name ) :\n",
        "        if not os.path.exists( dir_name ) : os.mkdir( dir_name )\n",
        "\n",
        "    def __download_xbrl_file( self ) :\n",
        "        counter ,retry = 0 ,3\n",
        "        while True :\n",
        "            try : \n",
        "                urllib.request.urlretrieve( self.xbrl_url ,self.xbrl_filepath )\n",
        "                return True\n",
        "            except Exception as e : \n",
        "                if counter < retry :\n",
        "                    print( f'XBRLファイルのDLに失敗しました（{ counter }）{ self.xbrl_url }' )\n",
        "                    time.sleep( 3 )\n",
        "                    counter += 1\n",
        "                else :\n",
        "                    print( '上手く行かないので次の処理に移ります' )\n",
        "                    return False\n",
        "\n",
        "    def parse_xbrl( self ):\n",
        "        DL_is_succeeded = self.__download_xbrl_file()\n",
        "        if DL_is_succeeded :\n",
        "            with open( self.xbrl_filepath ,'r' ,encoding='utf-8' ) as of :\n",
        "                xbrl = XBRLParser.parse( of ) # beautiful soup type object\n",
        "        else : return\n",
        "        result_dicts = {}\n",
        "        _idx = 0\n",
        "        name_space = 'jp*'\n",
        "        for node in xbrl.find_all( name=re.compile( name_space+':*' ) ):\n",
        "            if self.ignore_pattern( node ) : \n",
        "                continue\n",
        "            row_dict = {}\n",
        "            row_dict['file_nm'] = self.xbrl_filepath.rsplit( os.sep ,1 )[1]\n",
        "            row_dict['element_id'] = node.name\n",
        "            row_dict['amount'] = node.string\n",
        "            for tag in custom_tag:\n",
        "                row_dict[tag] = self.get_attrib_value( node ,tag )\n",
        "            result_dicts[_idx] = row_dict\n",
        "            _idx += 1\n",
        "        return result_dicts\n",
        "\n",
        "    def ignore_pattern( self ,node ):\n",
        "        if 'xsi:nil' in node.attrs:\n",
        "            if node.attrs['xsi:nil']=='true'   : return True\n",
        "        if not isinstance( node.string ,str )  : return True #結果が空の場合は対象外にする\n",
        "        if str( node.string ).find(u'\\n') > -1 : return True # 改行コードが結果にある場合対象外にする\n",
        "        if u'textblock' in str( node.name )    : return True #text文字列は解析に利用できないため、対象外\n",
        "        return False\n",
        "\n",
        "    def get_attrib_value( self, node, attrib ):\n",
        "        if attrib in node.attrs.keys() : return node.attrs[ attrib ]\n",
        "        else : return None\n",
        "\n",
        "\n",
        "\n",
        "class parse_operator() :\n",
        "    def __init__( self ,df ,str_period ,base_dir ) :\n",
        "        self.df = df\n",
        "        self.str_period = str_period\n",
        "        self.base_path = base_dir\n",
        "\n",
        "    def __dump_file( self ,writer ,info_dicts ) :\n",
        "        _idx = 0\n",
        "        while _idx < len(info_dicts):\n",
        "            row_dict = info_dicts[_idx]\n",
        "            writer.writerow(row_dict)\n",
        "            _idx += 1\n",
        "\n",
        "    def xbrl_to_csv( self ) :\n",
        "        file = f'eggs_{ self.str_period }.csv'\n",
        "        with open( os.path.join( self.base_path ,file ) ,'w' ,encoding=encode_type ) as of :\n",
        "            resultCsvWriter = csv.DictWriter( of ,default_tag + custom_tag ,lineterminator='\\n' )\n",
        "            resultCsvWriter.writeheader()\n",
        "            for i in tqdm( range( len( self.df.index ) ) ) :\n",
        "                row = self.df.iloc[i]\n",
        "                xp = XbrlParser( row.to_dict() )\n",
        "                info_dicts = xp.parse_xbrl()\n",
        "                self.__dump_file( resultCsvWriter ,info_dicts )\n",
        "        print( 'completed conversions!!' )\n",
        "\n",
        "\n",
        "\n",
        "def xbrl_operator( start ,end ,loop_month=1 ,restart=0 ,base_dir=None ) :\n",
        "    big_period = start.strftime('%y%m%d_')+end.strftime('%y%m%d')\n",
        "    base_path = f'{ os.getcwd() if base_dir==None else base_dir }'\n",
        "    file = f'dat_download_{ big_period }.csv'\n",
        "    info_df = pd.read_csv( os.path.join( base_path ,file ) ,parse_dates=['update'] )\n",
        "    since = start + relativedelta( months=( restart ) )\n",
        "    until = start + relativedelta( months=( loop_month + restart ) )\n",
        "    until = until if until < end else end\n",
        "\n",
        "    while True :\n",
        "        target_df = info_df.loc[ ( info_df['update'] >= since ) & ( info_df['update'] < until ) ]\n",
        "        small_period = since.strftime('%y%m%d_')+until.strftime('%y%m%d')\n",
        "        print( f'since:{ since } ,until:{ until } ({ small_period })' )\n",
        "        xbrl_parse_operator = parse_operator( target_df ,small_period ,base_path )\n",
        "        xbrl_parse_operator.xbrl_to_csv()\n",
        "        since = until\n",
        "        if since >= end : break\n",
        "        until = until + relativedelta( months=loop_month )\n",
        "        until = until if until < end else end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiMIXrqPYVTq",
        "colab_type": "code",
        "outputId": "0cd61214-3d92-4b91-f83d-95c08f4244eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "start = datetime.strptime('2020-04-01 +0900' ,'%Y-%m-%d %z')\n",
        "end   = datetime.strptime('2020-05-01 +0900' ,'%Y-%m-%d %z')\n",
        "# xbrl_operator( start ,end ,3 ,base_dir=BASE_DIR )\n",
        "xbrl_operator( start ,end ,3 )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/119 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "since:2020-04-01 00:00:00+09:00 ,until:2020-05-01 00:00:00+09:00 (200401_200501)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 119/119 [09:43<00:00,  4.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "completed conversions!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}